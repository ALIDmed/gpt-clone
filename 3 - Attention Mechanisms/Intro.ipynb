{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c75fb8f",
   "metadata": {},
   "source": [
    "# Why we need Attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c942c10b",
   "metadata": {},
   "source": [
    "## The problem with modeling long sequences\n",
    "\n",
    "Well, before the attention mechanism, architectures like RNN (encoder + decoder) were used in a sequence to sequence tasks like machine translation, however they had some noticeable problems.\n",
    "The encoder part of the architecture takes the each part of the input, then processes it into a hidden state (memory cell), the decoder then takes the hidden state to produce the output (the hidden state here plays the role of the embeddings).\n",
    "\n",
    "The issue here is that the decoder cannot access earlier hidden state during the decoding phase so it solely relies on the **current** hidden state.\n",
    "\n",
    "And this was the reason the **Attetion mechanism** was introduced.\n",
    "\n",
    "## Capturing data dependencies\n",
    "\n",
    "One shortcoming for the RNN is that it must remember entire encoded text in a single hidden state before passing it to the decoder\n",
    "*Bahdanau attention* was introduces that made the decoder selectively access different parts of the input sequence at each step \n",
    "\n",
    "The transformer architecture came in afterwards with a self-attention mechanism inspired by the *Bahdanau attention mechanism*\n",
    " - Self-attention is a mechanism that allows each position in the input sequence to attend to all positions in the same sequence when computing the representation of a sequence.\n",
    "\n",
    "\n",
    "## Attending to different parts of the input with self-attention\n",
    "\n",
    "in self-attention, the term “self” means that each element of a sequence (e.g., a word) pays attention to other elements within the same sequence to understand context and relationships. It learns how different parts of the input relate to each other, unlike traditional attention, which models relationships between two different sequences (e.g., input and output in seq2seq models)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff23cb61",
   "metadata": {},
   "source": [
    "### Simple self-attention mechanism without trainable weights\n",
    "\n",
    "In self-attention, our goal is to calcualte a context vector z<sup>(i)</sup> for each element x<sup>(i)</sup> in the input sequence. A context vector can be interpreted as an enriched embedding vector. The importance or contribution of each input element for computing z<sup>2</sup> for example is determined by the attention weights α<sub>21</sub> to α<sub>2T</sub>. When computing z<sup>2</sup>, the attention weights are calculated with respect to input element x<sup>2</sup> and all other inputs.\n",
    "\n",
    "<img src=\"self-attention.png\" width=\"600\">\n",
    "\n",
    "This enhanced context vector, z<sup>2</sup>, is an embedding that contains information about x<sup>(2)</sup> and all other input elements x<sup>(1)</sup>to x<sup>(T)</sup>.\n",
    "\n",
    "In self-attention, context vectors play a crucial role. Their purpose is to create enriched representations of each element in an input sequence (like a sentence) by incorporating\n",
    "information from all other elements in the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "25dcda28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "    [\n",
    "        [0.43, 0.15, 0.89],  # Your (x^1)\n",
    "        [0.55, 0.87, 0.66],  # journey (x^2)\n",
    "        [0.57, 0.85, 0.64],  # starts (x^3)\n",
    "        [0.22, 0.58, 0.33],  # with (x^4)\n",
    "        [0.77, 0.25, 0.10],  # one (x^5)\n",
    "        [0.05, 0.80, 0.55],\n",
    "    ]  # step (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8cb61d",
   "metadata": {},
   "source": [
    "<img src=\"attention-weight.png\" width=\"650\">\n",
    "\n",
    "We calculate the intermediate attention scores between the query token and each input token. We determine these scores by computing the dot product of the query, x<sup>2</sup> , with every other input token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5f84d0ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = inputs[1]\n",
    "\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(query, x_i)\n",
    "\n",
    "attn_scores_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d36f5f9",
   "metadata": {},
   "source": [
    "After computing the attention scores ω<sub>21</sub> to ω<sub>2T</sub> with respect to the input query x<sup>(2)</sup> , the next step is to obtain the attention weights α<sub>21</sub> to α<sub>2T</sub> by normalizing the attention scores\n",
    "The goal is to obtain attention weights that sum up to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "611eb0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: 1.0000001192092896\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "\n",
    "print(f\"Attention weights: {attn_weights_2_tmp}\")\n",
    "print(f\"Sum: {attn_weights_2_tmp.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8931a594",
   "metadata": {},
   "source": [
    "In practice, it's more common and advisable to use the softmax function for normalization. This approach is better at managing extreme values and offers more favorable gradient properties during training, the softmax function ensures that the attention weights are always positive.\n",
    "This makes the output interpretable as probabilities or relative importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "286df335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: 1.0\n"
     ]
    }
   ],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "\n",
    "print(f\"Attention weights: {attn_weights_2_naive}\")\n",
    "print(f\"Sum: {attn_weights_2_naive.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95e7777",
   "metadata": {},
   "source": [
    "This softmax we implemented may encounter numerical instability problems, such as overflow or underflow when dealing with large or small inputs, in practice is advisable to use pytoch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3c7d27c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: 1.0\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "\n",
    "print(f\"Attention weights: {attn_weights_2}\")\n",
    "print(f\"Sum: {attn_weights_2.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9b5fb0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] # 2nd input token is the query\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "print(context_vec_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d093c67",
   "metadata": {},
   "source": [
    "### Computing attention weights for all input tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7a2325af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5434706d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.empty(6, 6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b50e0f6",
   "metadata": {},
   "source": [
    "because python loops are generally slow, we need to use matrix multiplication to achieve the same results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "de32c021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4566bc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "32ce623c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "75feeff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "all_context_vector = attn_weights @ inputs\n",
    "print(all_context_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df66424",
   "metadata": {},
   "source": [
    "## Implementing self-attention with trainable weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bafde0",
   "metadata": {},
   "source": [
    "We will implement the self-attention mechanism step by step by introducing the three trainable weight matrices W<sub>q</sub>, W<sub>k</sub> and W<sub>v</sub>\n",
    "\n",
    "<img src=\"qkv.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "784e5ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]           # Second input element\n",
    "d_in = inputs.shape[1]    # Input embedding size\n",
    "d_out = 2               # Output embedding size\n",
    "\n",
    "# NOTE: in gpt-like models the input/output embedding sizes are usually the same, but for better illustration we chose different values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "95651a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query matrix weights:  Parameter containing:\n",
      "tensor([[0.2961, 0.5166],\n",
      "        [0.2517, 0.6886],\n",
      "        [0.0740, 0.8665]])\n",
      "Key matrix weights:  Parameter containing:\n",
      "tensor([[0.2961, 0.5166],\n",
      "        [0.2517, 0.6886],\n",
      "        [0.0740, 0.8665]])\n",
      "Value matrix weights:  Parameter containing:\n",
      "tensor([[0.0756, 0.1966],\n",
      "        [0.3164, 0.4017],\n",
      "        [0.1186, 0.8274]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "\n",
    "print(\"Query matrix weights: \", W_query)\n",
    "print(\"Key matrix weights: \", W_query)\n",
    "print(\"Value matrix weights: \", W_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "37291f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5500, 0.8700, 0.6600])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f944121d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4f7425f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4300, 0.1500, 0.8900],\n",
       "         [0.5500, 0.8700, 0.6600],\n",
       "         [0.5700, 0.8500, 0.6400],\n",
       "         [0.2200, 0.5800, 0.3300],\n",
       "         [0.7700, 0.2500, 0.1000],\n",
       "         [0.0500, 0.8000, 0.5500]]),\n",
       " Parameter containing:\n",
       " tensor([[0.1366, 0.1025],\n",
       "         [0.1841, 0.7264],\n",
       "         [0.3153, 0.6871]]))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, W_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "aa4c372c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys shape:  torch.Size([6, 2])\n",
      "Values shape:  torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "\n",
    "print(\"Keys shape: \", keys.shape)\n",
    "print(\"Values shape: \", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aedc272",
   "metadata": {},
   "source": [
    "The second step is now to compute the attention scores, as shown\n",
    "\n",
    "<img src=\"attn_score_qkv.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "443ea163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "attn_score_22 = keys[1].dot(query_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3ee9bbb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4306, 1.4551])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6fa475c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3669, 0.4433, 0.4361, 0.2408, 0.1827, 0.3275],\n",
       "        [0.7646, 1.1419, 1.1156, 0.6706, 0.3292, 0.9642]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5bbc2057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492efb8b",
   "metadata": {},
   "source": [
    "Now we need to normalize the attention scores to obtain the attention weights\n",
    "\n",
    "<img src=\"attn_score_norm_qkv.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e4a1b0",
   "metadata": {},
   "source": [
    "we compute the attention weights by scaling the attention scores and using the softmax function we used earlier. The difference to earlier is that we now scale the attention scores by dividing them by the square root of the embedding dimension of the keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3a5cadfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb15ae1b",
   "metadata": {},
   "source": [
    "The reason for the normalization by the embedding dimension size is to improve the training performance by avoiding small gradients. For instance, when scaling up the embedding dimension, which is typically greater than thousand for GPT-like LLMs, large dot products can result in very small gradients during backpropagation due to the softmax function applied to them. As dot products increase, the softmax function\n",
    "behaves more like a step function, resulting in gradients nearing zero. These small gradients can drastically slow down learning or cause training to stagnate.\n",
    "\n",
    "The scaling by the square root of the embedding dimension is the reason why this self-attention mechanism is also called scaled-dot product attention.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fd2623",
   "metadata": {},
   "source": [
    "Now, the final step is to compute the context vectors\n",
    "\n",
    "<img src=\"context_vec_qkv.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "bc26f23a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1855, 0.8812],\n",
       "        [0.3951, 1.0037],\n",
       "        [0.3879, 0.9831],\n",
       "        [0.2393, 0.5493],\n",
       "        [0.1492, 0.3346],\n",
       "        [0.3221, 0.7863]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "88fa715e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "2c45dce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3061, 0.8210])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "context_vec_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700b2154",
   "metadata": {},
   "source": [
    "### Implementing a compact self-attention Python class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "80eb20e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttentionV1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = x @ self.W_query\n",
    "        keys    = x @ self.W_key\n",
    "        values  = x @ self.W_value\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6e6931cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "sa_v1 = SelfAttentionV1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4fbf45",
   "metadata": {},
   "source": [
    "Self-attention mechanism summary\n",
    "\n",
    "<img src=\"att_weight_kqv_summary.png\" width=\"650\" > "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b3a87b",
   "metadata": {},
   "source": [
    "We can improve the SelfAttentionV1 By using PyTorch's `nn.Linear` layers, which effectively perform matrix multiplication when the bias units are disabled. Additionally, a significant advantage of using nn.Linear instead of `nn.Parameter(torch.rand(...))` is that nn.Linear has an **optimized weight initialization scheme, contributing to more stable and effective model training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "34c4ac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttentionV2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = self.W_query(x)\n",
    "        keys    = self.W_key(x)\n",
    "        values  = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f50bcf28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttentionV2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c17764f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
